{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn, einsum\n",
    "import seaborn as sns\n",
    "import scipy.io as sio\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "from einops import rearrange, repeat\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "# !nvidia-smi\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min:  -10.0\n",
      "max:  10.000002\n",
      "(486, 512, 23)\n",
      "(486, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# load data\n",
    "eeg = np.load('sliced/slices.npy')\n",
    "# normalize data use minmaxscaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-10, 10))\n",
    "eeg = scaler.fit_transform(eeg.reshape(-1, 23)).reshape(eeg.shape)\n",
    "print('min: ', np.min(eeg))\n",
    "print('max: ', np.max(eeg))\n",
    "\n",
    "# print(eeg.shape)  # expected to be (54, 23, 10, 256)\n",
    "\n",
    "# create list to store the pairs\n",
    "pair_list = []\n",
    "label_list = []\n",
    "\n",
    "# loop over patients\n",
    "for patient in eeg:\n",
    "    patient_pairs = []\n",
    "    patient_labels = []\n",
    "    # loop over segments within each patient\n",
    "    for i in range(9):  # 9 pairs for 10 segments\n",
    "        # concatenate segments along the time axis\n",
    "        pair = np.concatenate([patient[:, i, :], patient[:, i+1, :]], axis=1)\n",
    "        patient_pairs.append(pair)\n",
    "        patient_labels.append(i)\n",
    "    pair_list.append(patient_pairs)\n",
    "    label_list.append(patient_labels)\n",
    "\n",
    "# convert lists to numpy arrays\n",
    "pair_array = np.array(pair_list).reshape(54*9, 23, 512)\n",
    "pair_array = np.transpose(pair_array, (0, 2, 1))\n",
    "label_array = np.array(label_list).reshape(54*9, 1)\n",
    "\n",
    "print(pair_array.shape)  # should be (54, 9, 23, 512)\n",
    "print(label_array.shape)  # should be (54, 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 437 437\n",
      "test data: 49 49\n"
     ]
    }
   ],
   "source": [
    "class Mydatasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, data,label ,transform = None):\n",
    "        self.transform = transform\n",
    "\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.datanum = len(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        out_data = torch.tensor(self.data[idx]).float()\n",
    "        out_label = torch.tensor(self.label[idx])\n",
    "        if self.transform:\n",
    "            out_data = self.transform(out_data)\n",
    "\n",
    "        return out_data, out_label\n",
    "    \n",
    "train, test, train_label, test_label = train_test_split(pair_array, np.array(label_array),test_size = 0.1,stratify=np.array(label_array), random_state = 0)\n",
    "print('train data:',len(train),len(train_label))\n",
    "print('test data:',len(test),len(test_label))\n",
    "\n",
    "train_data_set = Mydatasets(data=train,label=train_label)\n",
    "test_data_set = Mydatasets(data=test,label=test_label)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data_set, batch_size = 16, shuffle=True, drop_last=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data_set, batch_size = 16, shuffle=False, drop_last=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU\n",
    "class CPC(nn.Module):\n",
    "    def __init__ (self, input_layer,hidden_layer, n_layer, num_class,group, time_step):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.time_step = time_step # k  ___learn___{   len=time_step   }\n",
    "        # input size: (Bch, len, chnl)\n",
    "        # nn.GRU: (in=chnl, out=>chnl2, GRU layers)\n",
    "        self.gru_first = nn.GRU(23, 128, 3, batch_first = True)#(bs, 256, 22) -> (bs, 256, 128)\n",
    "   \n",
    "        self.gru = nn.GRU(input_layer,hidden_layer,n_layer,batch_first = True)\n",
    "        \n",
    "        self.dec = nn.ModuleList([nn.Linear(hidden_layer,128) for i in range(time_step)])# k=T, could!=, \n",
    "        \n",
    "        self.mlp_head = nn.Linear(hidden_layer,num_class)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.lsoftmax = nn.LogSoftmax(dim=1) # loss func \n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        batch = x.shape[0] \n",
    "                \n",
    "        t_samples = 255 # where to start: ___learn___{ }____pred____\n",
    "\n",
    "        z,__ = self.gru_first(x) # z:(bs, 256, 128)\n",
    "        \n",
    "        nce = 0\n",
    "        \n",
    "        encoder_sample = torch.empty((self.time_step,batch,128)).float() # Z_{t+k}; ()\n",
    "        \n",
    "        for i in np.arange(1,self.time_step+1):\n",
    "            encoder_sample[i-1] = z[:,t_samples+i,:].view(batch,128) #from start to start+k\n",
    "        \n",
    "        forward_seq = z[:,:t_samples+1,:] # Z_{t}, {  forward_seq  }_____pred______\n",
    "        \n",
    "        output,hidden = self.gru(forward_seq) # decrease dim, (bs, 256-128, 128) --> (bs, 256-128, 64)\n",
    "        c_t = output[:,t_samples,:].view(batch,64)# at T time, (bs, 128th, 64) --> (bs, 64)\n",
    "        \n",
    "        pred = torch.empty((self.time_step,batch,128)).float() # W_{k}C_{t}, empty(128, bs, 128)\n",
    "        for i in np.arange(0,self.time_step): # do time_step=128 times linear\n",
    "            linear = self.dec[i]\n",
    "            pred[i] = linear(c_t) # (bs, 64)--> (bs, 128)\n",
    "        for i in np.arange(0,self.time_step):\n",
    "            total = torch.mm(encoder_sample[i], torch.transpose(pred[i],0,1)) # positive samples\n",
    "            correct = torch.sum(torch.eq(torch.argmax(self.softmax(total),dim=0),torch.arange(0,batch))) # pred, label\n",
    "            nce += torch.sum(torch.diag(self.lsoftmax(total)))\n",
    "        nce /= -1.*batch*self.time_step\n",
    "        acc = 1.*correct.item()/batch\n",
    "        \n",
    "        out = self.mlp_head(output[:,-1,:]) #for cls, ct or zt is ok.\n",
    "        return acc,nce,out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "CPC(\n",
      "  (gru_first): GRU(23, 128, num_layers=3, batch_first=True)\n",
      "  (gru): GRU(128, 64, batch_first=True)\n",
      "  (dec): ModuleList(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (3): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (4): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (5): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (6): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (7): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (8): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (9): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (10): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (11): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (12): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (13): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (14): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (15): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (16): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (17): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (18): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (19): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (20): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (21): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (22): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (23): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (24): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (25): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (26): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (27): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (28): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (29): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (30): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (31): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (32): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (33): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (34): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (35): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (36): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (37): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (38): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (39): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (40): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (41): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (42): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (43): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (44): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (45): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (46): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (47): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (48): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (49): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (50): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (51): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (52): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (53): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (54): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (55): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (56): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (57): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (58): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (59): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (60): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (61): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (62): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (63): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (64): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (65): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (66): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (67): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (68): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (69): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (70): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (71): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (72): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (73): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (74): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (75): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (76): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (77): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (78): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (79): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (80): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (81): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (82): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (83): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (84): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (85): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (86): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (87): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (88): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (89): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (90): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (91): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (92): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (93): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (94): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (95): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (96): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (97): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (98): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (99): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (100): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (101): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (102): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (103): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (104): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (105): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (106): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (107): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (108): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (109): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (110): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (111): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (112): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (113): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (114): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (115): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (116): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (117): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (118): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (119): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (120): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (121): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (122): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (123): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (124): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (125): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (126): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (127): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (128): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (129): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (130): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (131): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (132): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (133): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (134): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (135): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (136): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (137): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (138): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (139): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (140): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (141): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (142): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (143): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (144): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (145): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (146): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (147): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (148): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (149): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (150): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (151): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (152): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (153): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (154): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (155): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (156): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (157): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (158): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (159): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (160): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (161): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (162): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (163): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (164): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (165): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (166): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (167): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (168): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (169): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (170): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (171): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (172): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (173): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (174): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (175): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (176): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (177): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (178): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (179): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (180): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (181): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (182): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (183): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (184): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (185): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (186): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (187): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (188): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (189): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (190): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (191): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (192): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (193): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (194): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (195): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (196): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (197): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (198): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (199): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (200): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (201): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (202): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (203): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (204): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (205): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (206): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (207): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (208): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (209): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (210): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (211): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (212): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (213): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (214): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (215): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (216): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (217): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (218): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (219): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (220): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (221): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (222): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (223): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (224): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (225): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (226): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (227): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (228): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (229): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (230): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (231): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (232): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (233): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (234): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (235): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (236): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (237): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (238): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (239): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (240): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (241): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (242): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (243): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (244): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (245): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (246): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (247): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (248): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (249): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (250): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (251): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (252): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (253): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (254): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (255): Linear(in_features=64, out_features=128, bias=True)\n",
      "  )\n",
      "  (mlp_head): Linear(in_features=64, out_features=9, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      "  (lsoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "2424649\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\"if torch.cuda.is_available() else \"cpu\")\n",
    "CPC_model = CPC(\n",
    "    input_layer=128, # LSTM input\n",
    "    hidden_layer=64, # LSTM dim\n",
    "    n_layer=1, # LSTM layers\n",
    "    num_class=9, \n",
    "    group=5, # wave band, no use\n",
    "    time_step= 256, # k, how long future you will predict\n",
    ").to(DEVICE)\n",
    "\n",
    "print(DEVICE)\n",
    "total = sum(p.numel() for p in CPC_model.parameters())\n",
    "print(CPC_model)\n",
    "print(total)\n",
    "\n",
    "optimizer = torch.optim.AdamW(CPC_model.parameters(), lr = 0.01)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:22<10:49, 22.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.186, Accuracy: 2.250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:44<10:25, 22.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.748, Accuracy: 2.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [01:06<09:59, 22.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.318, Accuracy: 2.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [01:28<09:32, 22.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.216, Accuracy: 2.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [01:49<09:06, 21.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.918, Accuracy: 2.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [02:11<08:42, 21.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.889, Accuracy: 2.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [02:33<08:21, 21.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.884, Accuracy: 2.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [02:55<07:58, 21.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.882, Accuracy: 2.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [03:17<07:38, 21.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.880, Accuracy: 2.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [03:39<07:17, 21.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.880, Accuracy: 2.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [04:00<06:55, 21.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.879, Accuracy: 2.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [04:22<06:33, 21.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.879, Accuracy: 2.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [04:45<06:13, 21.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.878, Accuracy: 2.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [05:07<05:52, 22.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.878, Accuracy: 2.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [05:29<05:31, 22.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.876, Accuracy: 2.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [05:51<05:08, 22.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.191, Accuracy: 1.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [06:13<04:46, 22.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.063, Accuracy: 1.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [06:35<04:25, 22.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.289, Accuracy: 1.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [06:57<04:02, 22.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.002, Accuracy: 2.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [07:19<03:39, 21.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.264, Accuracy: 1.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [07:40<03:16, 21.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.172, Accuracy: 1.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [08:03<02:55, 21.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.029, Accuracy: 1.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [08:24<02:32, 21.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.185, Accuracy: 2.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [08:46<02:11, 21.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.047, Accuracy: 1.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [09:08<01:49, 21.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.062, Accuracy: 1.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [09:31<01:28, 22.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.093, Accuracy: 1.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [09:53<01:06, 22.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.020, Accuracy: 1.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [10:15<00:44, 22.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.097, Accuracy: 1.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [10:36<00:21, 21.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.014, Accuracy: 1.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [10:58<00:00, 21.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.017, Accuracy: 1.667\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "# assuming that you have already defined CPC_model, DEVICE, and optimizer\n",
    "EPOCH = 30\n",
    "train_times = 0\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "loss_list=[]\n",
    "val_loss_list=[]\n",
    "ac_list=[]\n",
    "\n",
    "for epoch in tqdm.tqdm(range(EPOCH)):\n",
    "    running_loss = 0.0\n",
    "    count = 0\n",
    "    for  _, (inputs, labels) in enumerate(train_dataloader, 0):\n",
    "        # print(f'Batch: {i}, Inputs Size: {inputs.size()}, Labels Size: {labels.size()}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.type(torch.LongTensor)   # casting to long\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        acc, nce_loss, outputs = CPC_model(inputs)\n",
    "\n",
    "        clsfy_loss = loss_func(outputs, labels.squeeze()) # .squeeze()\n",
    "\n",
    "        total_loss = clsfy_loss + nce_loss\n",
    "\n",
    "        # Backpropagate the loss\n",
    "        total_loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += total_loss.item()\n",
    "        if count % 100 == 99:    # print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, count + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    # evaluation phase\n",
    "    CPC_model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for _, (inputs, labels) in enumerate(test_dataloader, 0):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            acc, nce_loss, outputs = CPC_model(inputs)\n",
    "            clsfy_loss = loss_func(outputs, labels.squeeze())\n",
    "\n",
    "            total_loss = clsfy_loss + nce_loss\n",
    "            val_loss += total_loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_preds += labels.size(0)\n",
    "            correct_preds += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(test_dataloader)\n",
    "        val_accuracy = correct_preds / total_preds\n",
    "\n",
    "        print('Validation Loss: {:.3f}, Accuracy: {:.3f}'.format(val_loss, val_accuracy))\n",
    "\n",
    "    CPC_model.train()  # Set the model back to training mode\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ee256d8bb0>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ed92b71340>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGdCAYAAADuR1K7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAf20lEQVR4nO3de3BU9f3/8deGkATFTcotayARbalEpNAGE8J0htbsGJSOpOKIGQSkGSkV0BpKAUUy2nbSilZQUMaZOgxVCoVaWpHi0GCVysoleOEWxnaUq5uAmA2iJDH5/P7wx9qVEMFvTpJ983zMnGE4+zm7n8+ZwD7ncHbxOeecAAAAjEjo6AkAAAC0JeIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAApiR29AQ6QnNzs44eParLLrtMPp+vo6cDAADOg3NOJ0+eVEZGhhISzn195qKMm6NHjyozM7OjpwEAAL6GQ4cOqV+/fud8/KKMm8suu0zS5yfH7/d38GwAAMD5qKurU2ZmZvR9/Fwuyrg5809Rfr+fuAEAIM581S0l3FAMAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADClXeJmyZIl6t+/v1JSUpSXl6dt27a1On716tUaOHCgUlJSNHjwYK1fv/6cY6dOnSqfz6eFCxe28awBAEA88jxuVq1apdLSUpWVlWnnzp0aMmSICgsLVVNT0+L4LVu2qLi4WCUlJXrzzTdVVFSkoqIi7d69+6yxf/3rX/XGG28oIyPD62UAAIA44Xnc/P73v9ddd92lyZMn65prrtHSpUt1ySWX6Nlnn21x/KJFizRq1CjNmjVL2dnZ+tWvfqXvfe97Wrx4ccy4I0eOaMaMGXr++efVtWtXr5cBAADihKdx09DQoMrKSgWDwS9eMCFBwWBQoVCoxWNCoVDMeEkqLCyMGd/c3KwJEyZo1qxZGjRo0FfOo76+XnV1dTEbAACwydO4OX78uJqampSenh6zPz09XeFwuMVjwuHwV47/3e9+p8TERN1zzz3nNY/y8nKlpqZGt8zMzAtcCQAAiBdx92mpyspKLVq0SMuWLZPP5zuvY+bOnatIJBLdDh065PEsAQBAR/E0bnr16qUuXbqouro6Zn91dbUCgUCLxwQCgVbHb968WTU1NcrKylJiYqISExN14MABzZw5U/3792/xOZOTk+X3+2M2AABgk6dxk5SUpJycHFVUVET3NTc3q6KiQvn5+S0ek5+fHzNekjZu3BgdP2HCBL3zzjt66623oltGRoZmzZqll19+2bvFAACAuJDo9QuUlpZq0qRJGjZsmHJzc7Vw4UKdOnVKkydPliRNnDhRffv2VXl5uSTp3nvv1ciRI/XYY49p9OjRWrlypXbs2KFnnnlGktSzZ0/17Nkz5jW6du2qQCCgq6++2uvlAACATs7zuBk3bpyOHTum+fPnKxwOa+jQodqwYUP0puGDBw8qIeGLC0gjRozQihUrNG/ePN1///0aMGCA1q5dq2uvvdbrqQIAAAN8zjnX0ZNob3V1dUpNTVUkEuH+GwAA4sT5vn/H3aelAAAAWkPcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwJR2iZslS5aof//+SklJUV5enrZt29bq+NWrV2vgwIFKSUnR4MGDtX79+uhjjY2Nmj17tgYPHqxLL71UGRkZmjhxoo4ePer1MgAAQBzwPG5WrVql0tJSlZWVaefOnRoyZIgKCwtVU1PT4vgtW7aouLhYJSUlevPNN1VUVKSioiLt3r1bkvTJJ59o586devDBB7Vz50698MIL2r9/v26++WavlwIAAOKAzznnvHyBvLw8XXfddVq8eLEkqbm5WZmZmZoxY4bmzJlz1vhx48bp1KlTWrduXXTf8OHDNXToUC1durTF19i+fbtyc3N14MABZWVlfeWc6urqlJqaqkgkIr/f/zVXBgAA2tP5vn97euWmoaFBlZWVCgaDX7xgQoKCwaBCoVCLx4RCoZjxklRYWHjO8ZIUiUTk8/mUlpbW4uP19fWqq6uL2QAAgE2exs3x48fV1NSk9PT0mP3p6ekKh8MtHhMOhy9o/OnTpzV79mwVFxefs+LKy8uVmpoa3TIzM7/GagAAQDyI609LNTY26rbbbpNzTk8//fQ5x82dO1eRSCS6HTp0qB1nCQAA2lOil0/eq1cvdenSRdXV1TH7q6urFQgEWjwmEAic1/gzYXPgwAFt2rSp1X97S05OVnJy8tdcBQAAiCeeXrlJSkpSTk6OKioqovuam5tVUVGh/Pz8Fo/Jz8+PGS9JGzdujBl/Jmzeffdd/fOf/1TPnj29WQAAAIg7nl65kaTS0lJNmjRJw4YNU25urhYuXKhTp05p8uTJkqSJEyeqb9++Ki8vlyTde++9GjlypB577DGNHj1aK1eu1I4dO/TMM89I+jxsbr31Vu3cuVPr1q1TU1NT9H6cHj16KCkpyeslAQCATszzuBk3bpyOHTum+fPnKxwOa+jQodqwYUP0puGDBw8qIeGLC0gjRozQihUrNG/ePN1///0aMGCA1q5dq2uvvVaSdOTIEf3973+XJA0dOjTmtV555RX94Ac/8HpJAACgE/P8e246I77nBgCA+NMpvucGAACgvRE3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMKVd4mbJkiXq37+/UlJSlJeXp23btrU6fvXq1Ro4cKBSUlI0ePBgrV+/PuZx55zmz5+vyy+/XN26dVMwGNS7777r5RIAAECc8DxuVq1apdLSUpWVlWnnzp0aMmSICgsLVVNT0+L4LVu2qLi4WCUlJXrzzTdVVFSkoqIi7d69OzrmkUce0RNPPKGlS5dq69atuvTSS1VYWKjTp097vRwAANDJ+ZxzzssXyMvL03XXXafFixdLkpqbm5WZmakZM2Zozpw5Z40fN26cTp06pXXr1kX3DR8+XEOHDtXSpUvlnFNGRoZmzpypX/ziF5KkSCSi9PR0LVu2TLfffvtXzqmurk6pqamKRCLy+/1ttFIAAOCl833/9vTKTUNDgyorKxUMBr94wYQEBYNBhUKhFo8JhUIx4yWpsLAwOv69995TOByOGZOamqq8vLxzPmd9fb3q6upiNgAAYJOncXP8+HE1NTUpPT09Zn96errC4XCLx4TD4VbHn/n1Qp6zvLxcqamp0S0zM/NrrQcAAHR+F8WnpebOnatIJBLdDh061NFTAgAAHvE0bnr16qUuXbqouro6Zn91dbUCgUCLxwQCgVbHn/n1Qp4zOTlZfr8/ZgMAADZ5GjdJSUnKyclRRUVFdF9zc7MqKiqUn5/f4jH5+fkx4yVp48aN0fFXXnmlAoFAzJi6ujpt3br1nM8JAAAuHolev0BpaakmTZqkYcOGKTc3VwsXLtSpU6c0efJkSdLEiRPVt29flZeXS5LuvfdejRw5Uo899phGjx6tlStXaseOHXrmmWckST6fTz//+c/161//WgMGDNCVV16pBx98UBkZGSoqKvJ6OQAAoJPzPG7GjRunY8eOaf78+QqHwxo6dKg2bNgQvSH44MGDSkj44gLSiBEjtGLFCs2bN0/333+/BgwYoLVr1+raa6+NjvnlL3+pU6dOacqUKaqtrdX3v/99bdiwQSkpKV4vBwAAdHKef89NZ8T33AAAEH86xffcAAAAtDfiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKZ4FjcnTpzQ+PHj5ff7lZaWppKSEn388cetHnP69GlNmzZNPXv2VPfu3TV27FhVV1dHH3/77bdVXFyszMxMdevWTdnZ2Vq0aJFXSwAAAHHIs7gZP3689uzZo40bN2rdunV67bXXNGXKlFaPue+++/Tiiy9q9erVevXVV3X06FHdcsst0ccrKyvVp08fPffcc9qzZ48eeOABzZ07V4sXL/ZqGQAAIM74nHOurZ903759uuaaa7R9+3YNGzZMkrRhwwbddNNNOnz4sDIyMs46JhKJqHfv3lqxYoVuvfVWSVJVVZWys7MVCoU0fPjwFl9r2rRp2rdvnzZt2nTe86urq1NqaqoikYj8fv/XWCEAAGhv5/v+7cmVm1AopLS0tGjYSFIwGFRCQoK2bt3a4jGVlZVqbGxUMBiM7hs4cKCysrIUCoXO+VqRSEQ9evRou8kDAIC4lujFk4bDYfXp0yf2hRIT1aNHD4XD4XMek5SUpLS0tJj96enp5zxmy5YtWrVqlV566aVW51NfX6/6+vro7+vq6s5jFQAAIB5d0JWbOXPmyOfztbpVVVV5NdcYu3fv1pgxY1RWVqYbbrih1bHl5eVKTU2NbpmZme0yRwAA0P4u6MrNzJkzdeedd7Y65qqrrlIgEFBNTU3M/s8++0wnTpxQIBBo8bhAIKCGhgbV1tbGXL2prq4+65i9e/eqoKBAU6ZM0bx5875y3nPnzlVpaWn093V1dQQOAABGXVDc9O7dW7179/7Kcfn5+aqtrVVlZaVycnIkSZs2bVJzc7Py8vJaPCYnJ0ddu3ZVRUWFxo4dK0nav3+/Dh48qPz8/Oi4PXv26Prrr9ekSZP0m9/85rzmnZycrOTk5PMaCwAA4psnn5aSpBtvvFHV1dVaunSpGhsbNXnyZA0bNkwrVqyQJB05ckQFBQVavny5cnNzJUk/+9nPtH79ei1btkx+v18zZsyQ9Pm9NdLn/xR1/fXXq7CwUAsWLIi+VpcuXc4rus7g01IAAMSf833/9uSGYkl6/vnnNX36dBUUFCghIUFjx47VE088EX28sbFR+/fv1yeffBLd9/jjj0fH1tfXq7CwUE899VT08TVr1ujYsWN67rnn9Nxzz0X3X3HFFXr//fe9WgoAAIgjnl256cy4cgMAQPzp0O+5AQAA6CjEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCmexc2JEyc0fvx4+f1+paWlqaSkRB9//HGrx5w+fVrTpk1Tz5491b17d40dO1bV1dUtjv3www/Vr18/+Xw+1dbWerACAAAQjzyLm/Hjx2vPnj3auHGj1q1bp9dee01Tpkxp9Zj77rtPL774olavXq1XX31VR48e1S233NLi2JKSEn3nO9/xYuoAACCO+Zxzrq2fdN++fbrmmmu0fft2DRs2TJK0YcMG3XTTTTp8+LAyMjLOOiYSiah3795asWKFbr31VklSVVWVsrOzFQqFNHz48OjYp59+WqtWrdL8+fNVUFCgjz76SGlpaec9v7q6OqWmpioSicjv9//fFgsAANrF+b5/e3LlJhQKKS0tLRo2khQMBpWQkKCtW7e2eExlZaUaGxsVDAaj+wYOHKisrCyFQqHovr179+rhhx/W8uXLlZBwftOvr69XXV1dzAYAAGzyJG7C4bD69OkTsy8xMVE9evRQOBw+5zFJSUlnXYFJT0+PHlNfX6/i4mItWLBAWVlZ5z2f8vJypaamRrfMzMwLWxAAAIgbFxQ3c+bMkc/na3Wrqqryaq6aO3eusrOzdccdd1zwcZFIJLodOnTIoxkCAICOlnghg2fOnKk777yz1TFXXXWVAoGAampqYvZ/9tlnOnHihAKBQIvHBQIBNTQ0qLa2NubqTXV1dfSYTZs2adeuXVqzZo0k6cztQr169dIDDzyghx56qMXnTk5OVnJy8vksEQAAxLkLipvevXurd+/eXzkuPz9ftbW1qqysVE5OjqTPw6S5uVl5eXktHpOTk6OuXbuqoqJCY8eOlSTt379fBw8eVH5+viTpL3/5iz799NPoMdu3b9dPfvITbd68Wd/85jcvZCkAAMCoC4qb85Wdna1Ro0bprrvu0tKlS9XY2Kjp06fr9ttvj35S6siRIyooKNDy5cuVm5ur1NRUlZSUqLS0VD169JDf79eMGTOUn58f/aTUlwPm+PHj0de7kE9LAQAAuzyJG0l6/vnnNX36dBUUFCghIUFjx47VE088EX28sbFR+/fv1yeffBLd9/jjj0fH1tfXq7CwUE899ZRXUwQAAAZ58j03nR3fcwMAQPzp0O+5AQAA6CjEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMIW4AQAAphA3AADAFOIGAACYQtwAAABTiBsAAGAKcQMAAEwhbgAAgCnEDQAAMCWxoyfQEZxzkqS6uroOngkAADhfZ963z7yPn8tFGTcnT56UJGVmZnbwTAAAwIU6efKkUlNTz/m4z31V/hjU3Nyso0eP6rLLLpPP5+vo6XS4uro6ZWZm6tChQ/L7/R09HbM4z+2D89w+OM/tg/McyzmnkydPKiMjQwkJ576z5qK8cpOQkKB+/fp19DQ6Hb/fzx+edsB5bh+c5/bBeW4fnOcvtHbF5gxuKAYAAKYQNwAAwBTiBkpOTlZZWZmSk5M7eiqmcZ7bB+e5fXCe2wfn+eu5KG8oBgAAdnHlBgAAmELcAAAAU4gbAABgCnEDAABMIW4uAidOnND48ePl9/uVlpamkpISffzxx60ec/r0aU2bNk09e/ZU9+7dNXbsWFVXV7c49sMPP1S/fv3k8/lUW1vrwQrigxfn+e2331ZxcbEyMzPVrVs3ZWdna9GiRV4vpdNZsmSJ+vfvr5SUFOXl5Wnbtm2tjl+9erUGDhyolJQUDR48WOvXr4953Dmn+fPn6/LLL1e3bt0UDAb17rvvermEuNCW57mxsVGzZ8/W4MGDdemllyojI0MTJ07U0aNHvV5Gp9fWP8//a+rUqfL5fFq4cGEbzzrOOJg3atQoN2TIEPfGG2+4zZs3u29961uuuLi41WOmTp3qMjMzXUVFhduxY4cbPny4GzFiRItjx4wZ42688UYnyX300UcerCA+eHGe//CHP7h77rnH/etf/3L//e9/3R//+EfXrVs39+STT3q9nE5j5cqVLikpyT377LNuz5497q677nJpaWmuurq6xfGvv/6669Kli3vkkUfc3r173bx581zXrl3drl27omN++9vfutTUVLd27Vr39ttvu5tvvtldeeWV7tNPP22vZXU6bX2ea2trXTAYdKtWrXJVVVUuFAq53Nxcl5OT057L6nS8+Hk+44UXXnBDhgxxGRkZ7vHHH/d4JZ0bcWPc3r17nSS3ffv26L5//OMfzufzuSNHjrR4TG1trevatatbvXp1dN++ffucJBcKhWLGPvXUU27kyJGuoqLioo4br8/z/7r77rvdD3/4w7abfCeXm5vrpk2bFv19U1OTy8jIcOXl5S2Ov+2229zo0aNj9uXl5bmf/vSnzjnnmpubXSAQcAsWLIg+Xltb65KTk92f/vQnD1YQH9r6PLdk27ZtTpI7cOBA20w6Dnl1ng8fPuz69u3rdu/e7a644oqLPm74ZynjQqGQ0tLSNGzYsOi+YDCohIQEbd26tcVjKisr1djYqGAwGN03cOBAZWVlKRQKRfft3btXDz/8sJYvX97qf2B2MfDyPH9ZJBJRjx492m7ynVhDQ4MqKytjzlFCQoKCweA5z1EoFIoZL0mFhYXR8e+9957C4XDMmNTUVOXl5bV63i3z4jy3JBKJyOfzKS0trU3mHW+8Os/Nzc2aMGGCZs2apUGDBnkz+Thzcb8jXQTC4bD69OkTsy8xMVE9evRQOBw+5zFJSUln/QWUnp4ePaa+vl7FxcVasGCBsrKyPJl7PPHqPH/Zli1btGrVKk2ZMqVN5t3ZHT9+XE1NTUpPT4/Z39o5CofDrY4/8+uFPKd1XpznLzt9+rRmz56t4uLii/Y/gPTqPP/ud79TYmKi7rnnnrafdJwibuLUnDlz5PP5Wt2qqqo8e/25c+cqOztbd9xxh2ev0Rl09Hn+X7t379aYMWNUVlamG264oV1eE2gLjY2Nuu222+Sc09NPP93R0zGlsrJSixYt0rJly+Tz+Tp6Op1GYkdPAF/PzJkzdeedd7Y65qqrrlIgEFBNTU3M/s8++0wnTpxQIBBo8bhAIKCGhgbV1tbGXFWorq6OHrNp0ybt2rVLa9askfT5p08kqVevXnrggQf00EMPfc2VdS4dfZ7P2Lt3rwoKCjRlyhTNmzfva60lHvXq1UtdunQ565N6LZ2jMwKBQKvjz/xaXV2tyy+/PGbM0KFD23D28cOL83zGmbA5cOCANm3adNFetZG8Oc+bN29WTU1NzBX0pqYmzZw5UwsXLtT777/ftouIFx190w+8deZG1x07dkT3vfzyy+d1o+uaNWui+6qqqmJudP3Pf/7jdu3aFd2effZZJ8lt2bLlnHf9W+bVeXbOud27d7s+ffq4WbNmebeATiw3N9dNnz49+vumpibXt2/fVm/A/NGPfhSzLz8//6wbih999NHo45FIhBuK2/g8O+dcQ0ODKyoqcoMGDXI1NTXeTDzOtPV5Pn78eMzfxbt27XIZGRlu9uzZrqqqyruFdHLEzUVg1KhR7rvf/a7bunWr+/e//+0GDBgQ8xHlw4cPu6uvvtpt3bo1um/q1KkuKyvLbdq0ye3YscPl5+e7/Pz8c77GK6+8clF/Wso5b87zrl27XO/evd0dd9zhPvjgg+h2Mb1RrFy50iUnJ7tly5a5vXv3uilTpri0tDQXDoedc85NmDDBzZkzJzr+9ddfd4mJie7RRx91+/btc2VlZS1+FDwtLc397W9/c++8844bM2YMHwVv4/Pc0NDgbr75ZtevXz/31ltvxfz81tfXd8gaOwMvfp6/jE9LETcXhQ8//NAVFxe77t27O7/f7yZPnuxOnjwZffy9995zktwrr7wS3ffpp5+6u+++233jG99wl1xyifvxj3/sPvjgg3O+BnHjzXkuKytzks7arrjiinZcWcd78sknXVZWlktKSnK5ubnujTfeiD42cuRIN2nSpJjxf/7zn923v/1tl5SU5AYNGuReeumlmMebm5vdgw8+6NLT011ycrIrKChw+/fvb4+ldGpteZ7P/Ly3tP3vn4GLUVv/PH8ZceOcz7n/f7MEAACAAXxaCgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABM+X9JnGEujayxKgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ac_list[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELF learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:21<10:37, 21.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 :finished\n",
      "train_loss: 2.7726277068809226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:43<10:09, 21.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 :finished\n",
      "train_loss: 2.772595591015286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [01:05<09:49, 21.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 :finished\n",
      "train_loss: 2.772577921549479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [01:27<09:24, 21.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 :finished\n",
      "train_loss: 2.7725629806518555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [01:48<09:01, 21.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 :finished\n",
      "train_loss: 2.772538759090282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [02:10<08:40, 21.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 :finished\n",
      "train_loss: 2.7724891591955116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [02:31<08:18, 21.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 :finished\n",
      "train_loss: 2.7723347257684776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [02:53<07:58, 21.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 :finished\n",
      "train_loss: 2.7686413394080267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [03:16<07:44, 22.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 :finished\n",
      "train_loss: 2.7623581091562905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [03:39<07:28, 22.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 :finished\n",
      "train_loss: 2.7723663912879095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [04:02<07:05, 22.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 :finished\n",
      "train_loss: 2.7819307733465126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [04:25<06:48, 22.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 :finished\n",
      "train_loss: 2.77259150257817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [04:48<06:27, 22.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 :finished\n",
      "train_loss: 2.7725897983268455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [05:11<06:03, 22.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 :finished\n",
      "train_loss: 2.772589400962547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [05:33<05:39, 22.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 :finished\n",
      "train_loss: 2.772589021258884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [05:55<05:15, 22.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16 :finished\n",
      "train_loss: 2.7725890477498374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [06:17<04:51, 22.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 :finished\n",
      "train_loss: 2.7725888976344355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [06:40<04:28, 22.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18 :finished\n",
      "train_loss: 2.7725886503855386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [07:02<04:06, 22.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 :finished\n",
      "train_loss: 2.7725882795121937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [07:24<03:42, 22.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 :finished\n",
      "train_loss: 2.772588129396792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [07:46<03:19, 22.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21 :finished\n",
      "train_loss: 2.7725879351298013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [08:08<02:56, 22.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22 :finished\n",
      "train_loss: 2.772587952790437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [08:30<02:34, 22.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23 :finished\n",
      "train_loss: 2.7725877850144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [08:52<02:12, 22.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24 :finished\n",
      "train_loss: 2.772587670220269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [09:14<01:49, 21.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25 :finished\n",
      "train_loss: 2.7725876525596336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [09:36<01:27, 21.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26 :finished\n",
      "train_loss: 2.772587458292643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [09:58<01:06, 22.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27 :finished\n",
      "train_loss: 2.7725873434985124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [10:12<01:08, 22.68s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m acc,nce_loss,_ \u001b[39m=\u001b[39m CPC_model(inputs)\n\u001b[0;32m     19\u001b[0m losses \u001b[39m=\u001b[39m nce_loss\n\u001b[1;32m---> 20\u001b[0m losses\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     21\u001b[0m optimizer\u001b[39m.\u001b[39mstep() \n\u001b[0;32m     22\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m losses\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\comparch\\anaconda3\\envs\\CondaPy39\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\comparch\\anaconda3\\envs\\CondaPy39\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCH = 30\n",
    "train_times = 0\n",
    "\n",
    "loss_list=[]\n",
    "val_loss_list=[]\n",
    "ac_list=[]\n",
    "\n",
    "for epoch in tqdm.tqdm(range(EPOCH)):\n",
    "    running_loss = 0.0\n",
    "    count = 0\n",
    "    for  _, (inputs, labels) in enumerate(train_dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(DEVICE)\n",
    "\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        acc,nce_loss,_ = CPC_model(inputs)\n",
    "\n",
    "        losses = nce_loss\n",
    "        losses.backward()\n",
    "        optimizer.step() \n",
    "        running_loss += losses.item()\n",
    "        count = count+1\n",
    "    loss_loss = running_loss/count\n",
    "    loss_list.append(loss_loss)\n",
    "    print('epoch',epoch+1,':finished')\n",
    "    print('train_loss:',loss_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_list[1:])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_list[1:])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
